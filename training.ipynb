{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HairChangerAI_Training.ipynb\n",
    "\n",
    "# Section 1: Setup and Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diffusers==0.24.0 transformers accelerate safetensors\n",
    "!pip install kaggle\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Kaggle Dataset Download\n",
    "import os\n",
    "os.environ['KAGGLE_USERNAME'] = 'Chinsa'  \n",
    "os.environ['KAGGLE_KEY'] = 'd0f2c4a1b5e3b8f7c9d6a1e2c3e4f5a6'\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!kaggle datasets download -d kavyasreeb/hair-type-dataset\n",
    "!unzip -q hair-type-dataset.zip -d hair_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Visualize the Dataset\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"hair_dataset/Hair dataset/Hair dataset\"\n",
    "categories = os.listdir(data_path)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, category in enumerate(categories[:5]):\n",
    "    img_path = os.path.join(data_path, category, os.listdir(os.path.join(data_path, category))[0])\n",
    "    img = cv2.imread(img_path)[..., ::-1]\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(category)\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Setup Fine-Tuning Pipeline\n",
    "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    revision=\"fp16\",\n",
    "    use_auth_token=True  \n",
    ").to(\"cuda\")\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class HairDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.image_paths = []\n",
    "        self.prompts = []\n",
    "        self.transform = transform\n",
    "\n",
    "        for style in os.listdir(root_dir):\n",
    "            style_path = os.path.join(root_dir, style)\n",
    "            for img_name in os.listdir(style_path):\n",
    "                img_path = os.path.join(style_path, img_name)\n",
    "                self.image_paths.append(img_path)\n",
    "                self.prompts.append(f\"a person with {style.lower()} hair\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        prompt = self.prompts[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return {\"image\": image, \"prompt\": prompt}\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "dataset = HairDataset(data_path, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6: Fine-tune the Text Encoder + UNet (DreamBooth Style)\n",
    "from diffusers import DDPMScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import UNet2DConditionModel, AutoencoderKL\n",
    "from accelerate import Accelerator\n",
    "\n",
    "\n",
    "print(\"✅ Dataset ready. From here, continue DreamBooth-style fine-tuning or LoRA...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7: Save the Fine-tuned Model\n",
    "def save_model(pipe, save_path=\"hair_model\"):\n",
    "    pipe.save_pretrained(save_path)\n",
    "    print(f\"✅ Model saved to {save_path}\")\n",
    "\n",
    "save_model(pipe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 8: Load the Fine-tuned Model\n",
    "def load_model(model_path=\"hair_model\"):\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.float16\n",
    "    ).to(\"cuda\")\n",
    "    print(f\"✅ Model loaded from {model_path}\")\n",
    "    return pipe\n",
    "\n",
    "loaded_pipe = load_model()\n",
    "\n",
    "# Section 9: Generate Images with the Fine-tuned Model"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
